{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd \n",
    "from skimage.io import imread\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels ID  [   1    2    3 ..., 6281 6282 6283]\n",
      "xTrain shape:  (6283, 441)\n",
      "Train labels shape:  (6283, 2)\n",
      "First 5 labels: \n",
      "    ID Class\n",
      "0   1     n\n",
      "1   2     8\n",
      "2   3     T\n",
      "3   4     I\n",
      "4   5     R\n",
      "xTest shape:  (6220, 441)\n",
      "Classes:  ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H'\n",
      " 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z'\n",
      " 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r'\n",
      " 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "def read_data(typeData, labelsInfo, imageSize, path):\n",
    " #Intialize x  matrix\n",
    " x = np.zeros((labelsInfo.shape[0], imageSize))\n",
    "\n",
    " for (index, idImage) in enumerate(labelsInfo[\"ID\"]):\n",
    "  #Read image file\n",
    "  nameFile = \"{0}/{1}Resized21/{2}.Bmp\".format(path, typeData, idImage)\n",
    "  img = imread(nameFile, as_grey=True)\n",
    "\n",
    "  x[index, :] = np.reshape(img, (1, imageSize))\n",
    " return x\n",
    "\n",
    "imageSize = 441 # 21 x 21 pixels\n",
    "\n",
    "#Set location of data files , folders\n",
    "path = '../../'\n",
    "\n",
    "labelsInfoTrain = pd.read_csv(\"{0}/trainLabels.csv\".format(path))  #this is a pandas DataFrame\n",
    "\n",
    "pathTrain = '../../trainResized21/'\n",
    "#Read training matrix\n",
    "xTrain = read_data(\"train\", labelsInfoTrain, imageSize, path)\n",
    "\n",
    "pathTrain = '../../'\n",
    "#Read information about test data ( IDs ).\n",
    "labelsInfoTest = pd.read_csv(\"{0}/sampleSubmission.csv\".format(pathTrain))\n",
    "\n",
    "pathTest = '../../testResized21/'\n",
    "#Read test matrix\n",
    "xTest = read_data(\"test\", labelsInfoTest, imageSize, path)\n",
    "\n",
    "\n",
    "print('Train labels ID ', labelsInfoTrain[\"ID\"].values)\n",
    "print('xTrain shape: ', xTrain.shape)  # train set has 6283 21X21 pixel images\n",
    "print('Train labels shape: ', labelsInfoTrain.shape)\n",
    "print('First 5 labels: \\n' , labelsInfoTrain[0:5])\n",
    "print('xTest shape: ', xTest.shape)    # test set has 6220 21X21 pixel images\n",
    "print('Classes: ', np.unique(labelsInfoTrain['Class'].values))     # 62 classes\n",
    "uniqueLabels = np.unique(labelsInfoTrain['Class'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Reformat into a TensorFlow-friendly shape:\n",
    "\n",
    "    convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "    labels as float 1-hot encodings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yTrain shape  (6283,)\n",
      "yTrain 1-5  [ 49.   8.  29.  18.  27.]\n",
      "Training set (6283, 21, 21, 1) (6283, 62)\n",
      "Test set (6220, 21, 21, 1)\n",
      "First 3 training labels  [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "image_size = 21\n",
    "num_labels = 62\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "def reformatLabels(labels):\n",
    "    yTrainInternal = np.zeros(labels.shape[0])\n",
    "    for i in range(labels.shape[0]):\n",
    "        labelCounter = 0\n",
    "        for j, value in np.ndenumerate(uniqueLabels):\n",
    "            labelCounter += 1\n",
    "            if labels.ix[i, 'Class'] == value:\n",
    "               yTrainInternal[i] = labelCounter-1\n",
    "    return yTrainInternal\n",
    "    \n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "yTrain = reformatLabels(labelsInfoTrain)   # transfer labels to 1-hot vector of 62 dimensions \n",
    "print('yTrain shape ', yTrain.shape)\n",
    "print('yTrain 1-5 ', yTrain[0:5])\n",
    "train_dataset, train_labels = reformat(xTrain, yTrain)\n",
    "test_dataset, _   = reformat(xTest , yTrain)               # kaggle does not provide labels on test set\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape)\n",
    "print('First 3 training labels ', train_labels[0:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  acc =  (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "  tf.scalar_summary('accuracy', acc ) \n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (300, 21, 21, 1)\n",
      "hidden1 shape (300, 6, 6, 16)\n",
      "conv2 shape (300, 3, 3, 16)\n",
      "hidden2 shape (300, 3, 3, 16)\n",
      "reshaped hidden2 shape (300, 144)\n",
      "hidden3 shape (300, 64)\n",
      "conv3 shape (300, 62)\n",
      "logits  Tensor(\"FullyConnected2/add:0\", shape=(300, 62), dtype=float32)\n",
      "data shape (6220, 21, 21, 1)\n",
      "hidden1 shape (6220, 6, 6, 16)\n",
      "conv2 shape (6220, 3, 3, 16)\n",
      "hidden2 shape (6220, 3, 3, 16)\n",
      "reshaped hidden2 shape (6220, 144)\n",
      "hidden3 shape (6220, 64)\n",
      "conv3 shape (6220, 62)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 300\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  with tf.name_scope('Train_Input'):\n",
    "      tf_train_dataset = tf.placeholder(\n",
    "          tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  with tf.name_scope('Test_Input'):\n",
    "      tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  with tf.name_scope('Layer1_Weights'):\n",
    "      layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "      tf.histogram_summary(\"lay1_weights\", layer1_weights)\n",
    "#\n",
    "# from https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/pros/index.html\n",
    "# We can now implement our first layer. It will consist of convolution, followed by max pooling. \n",
    "# The convolutional will compute 32 features for each 5x5 patch. Its weight tensor will have a shape of [5, 5, 1, 32]. The first two dimensions are the patch size, the next is the number of input channels, and the last is the number of output channels. We will also have a bias vector with a component for each output channel.\n",
    "#\n",
    "#    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "#    b_conv1 = bias_variable([32])\n",
    "#\n",
    "  with tf.name_scope('Layer1_bias'):\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    lay1_bias = tf.histogram_summary(\"lay1_biases\",  layer1_biases)\n",
    "  with tf.name_scope('Layer2_Weights'):  \n",
    "      layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "      tf.histogram_summary(\"lay2_weights\", layer2_weights)\n",
    "  with tf.name_scope('Layer2_bias'):\n",
    "      layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "      tf.histogram_summary(\"lay2_biases\",  layer2_biases)\n",
    "  with tf.name_scope('Layer3_Weights'):        \n",
    "      layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "          [image_size // 7 * image_size // 7 * depth, num_hidden], stddev=0.1))\n",
    "      tf.histogram_summary(\"lay3_weights\", layer3_weights)\n",
    "\n",
    "  with tf.name_scope('Layer3_bias'):\n",
    "      layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "      tf.histogram_summary(\"lay3_biases\",  layer3_biases)\n",
    "  with tf.name_scope('Layer4_Weights'): \n",
    "      layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "          [num_hidden, num_labels], stddev=0.1))\n",
    "      tf.histogram_summary(\"FullConnect_weights\", layer4_weights)\n",
    "\n",
    "  with tf.name_scope('Layer4_bias'):\n",
    "      layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "      tf.histogram_summary(\"lay4_biases\",  layer4_biases)\n",
    "  # Model.\n",
    "  def model(data):\n",
    "    print(\"data shape \" + str(data.get_shape()))\n",
    "    with tf.name_scope(\"FirstConvolution\"):\n",
    "        conv1 = tf.nn.conv2d(data,   layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.name_scope(\"ReLU1\"):\n",
    "        hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "#    print(\"hidden1 shape \" + str(hidden1.get_shape()))\n",
    "#\n",
    "# LeNet5 has max_pool after relu\n",
    "#\n",
    "    with tf.name_scope(\"SecondMax_Pool\"):\n",
    "        hidden1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1],   [1, 2, 2, 1], padding='SAME')\n",
    "    print(\"hidden1 shape \" + str(hidden1.get_shape()))\n",
    "#\n",
    "    with tf.name_scope(\"SecondConvolution\"):\n",
    "        conv2 = tf.nn.conv2d(hidden1, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    print(\"conv2 shape \" + str(conv2.get_shape()))\n",
    "\n",
    "    with tf.name_scope(\"ReLU2\"):\n",
    "        hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    print(\"hidden2 shape \" + str(hidden2.get_shape()))\n",
    "\n",
    "    shape = hidden2.get_shape().as_list()\n",
    "    with tf.name_scope(\"Reshape\"):\n",
    "        reshape = tf.reshape(hidden2, [shape[0], shape[1] * shape[2] * shape[3]])  #concatenation?\n",
    "    print(\"reshaped hidden2 shape \" + str(reshape.get_shape()))\n",
    "    with tf.name_scope(\"FullyConnected1\"):\n",
    "        multReshape = tf.matmul(reshape, layer3_weights) \n",
    "    with tf.name_scope(\"ReLU3\"):\n",
    "        hidden3 = tf.nn.relu(multReshape + layer3_biases)\n",
    "    print(\"hidden3 shape \" + str(hidden3.get_shape()))\n",
    "    with tf.name_scope(\"FullyConnected2\"):\n",
    "        conv3 = tf.matmul(hidden3, layer4_weights) + layer4_biases \n",
    "        # Add summary ops to collect data\n",
    "\n",
    "    print(\"conv3 shape \" + str(conv3.get_shape()))\n",
    "    \n",
    "    # Add summary ops to collect data\n",
    "\n",
    "    return conv3\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  tf.histogram_summary(\"logits\", logits)\n",
    "  print('logits ', logits)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  with tf.name_scope('Regularizers'):   \n",
    "# L2 regularization for the fully connected parameters.\n",
    "      regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "                  tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) +\n",
    "                  tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) +\n",
    "                  tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 1e-3 * regularizers\n",
    "  tf.scalar_summary('loss', loss)\n",
    "# Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "  with tf.name_scope('IndexToData'): \n",
    "      batch = tf.Variable(0)\n",
    "  train_size = 100\n",
    "  # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "  with tf.name_scope('LearningRate'):   \n",
    "      learning_rate = tf.train.exponential_decay(\n",
    "          0.01,                # Base learning rate.\n",
    "          batch * batch_size,  # Current index into the dataset.\n",
    "          train_size,          # Decay step.\n",
    "          0.95,                # Decay rate.\n",
    "          staircase=True)\n",
    "  # Use simple momentum for the optimization.\n",
    "\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.505950\n",
      "Minibatch accuracy: 0.3%\n",
      "Minibatch loss at step 300: 3.832182\n",
      "Minibatch accuracy: 9.7%\n",
      "Minibatch loss at step 600: 3.773775\n",
      "Minibatch accuracy: 9.3%\n",
      "Minibatch loss at step 900: 3.414647\n",
      "Minibatch accuracy: 23.7%\n",
      "Minibatch loss at step 1200: 3.009533\n",
      "Minibatch accuracy: 29.7%\n",
      "Minibatch loss at step 1500: 2.288896\n",
      "Minibatch accuracy: 44.7%\n",
      "Minibatch loss at step 1800: 1.896545\n",
      "Minibatch accuracy: 54.3%\n",
      "Minibatch loss at step 2100: 1.623484\n",
      "Minibatch accuracy: 61.7%\n",
      "Minibatch loss at step 2400: 1.444540\n",
      "Minibatch accuracy: 70.0%\n",
      "Minibatch loss at step 2700: 1.228104\n",
      "Minibatch accuracy: 73.0%\n",
      "Minibatch loss at step 3000: 1.197356\n",
      "Minibatch accuracy: 74.0%\n",
      "Minibatch loss at step 3300: 1.097261\n",
      "Minibatch accuracy: 79.7%\n",
      "Minibatch loss at step 3600: 1.048910\n",
      "Minibatch accuracy: 78.0%\n",
      "Minibatch loss at step 3900: 1.096447\n",
      "Minibatch accuracy: 73.3%\n",
      "Minibatch loss at step 4200: 1.059758\n",
      "Minibatch accuracy: 76.7%\n",
      "Minibatch loss at step 4500: 1.823554\n",
      "Minibatch accuracy: 55.0%\n",
      "Minibatch loss at step 4800: 1.182806\n",
      "Minibatch accuracy: 75.7%\n",
      "data shape (6220, 21, 21, 1)\n",
      "hidden1 shape (6220, 6, 6, 16)\n",
      "conv2 shape (6220, 3, 3, 16)\n",
      "hidden2 shape (6220, 3, 3, 16)\n",
      "reshaped hidden2 shape (6220, 144)\n",
      "hidden3 shape (6220, 64)\n",
      "conv3 shape (6220, 62)\n",
      "[44 14 18 36 44 55 50 36 11 17 23 32 10 17 40 11 10 27 33 28]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    " \n",
    "  merged = tf.merge_all_summaries()\n",
    "  summary_writer = tf.train.SummaryWriter('log_GoogleStreetView_convolution/batchSize300', session.graph)\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "    \n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "\n",
    "    _, l, predictions, summaryP = session.run(\n",
    "      [optimizer, loss, train_prediction, merged], feed_dict=feed_dict)\n",
    "\n",
    "    if (step % 300 == 0):\n",
    "\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "\n",
    "      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "      run_metadata = tf.RunMetadata()\n",
    "\n",
    "    summary_writer.add_summary(summaryP, step)\n",
    "    \n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "# code save numpy arrays to CSV\n",
    "\n",
    "  indices = np.arange(6284, 12504)\n",
    "  predictions_file = open(\"test_prediction_GoogleStreetView.csv\", \"w\")\n",
    "  open_file_object = csv.writer(predictions_file)\n",
    "  open_file_object.writerow([\"Id\",\"Class\"])\n",
    "  output = test_prediction.eval()\n",
    "  print (np.argmax(output, 1)[0:20] ) \n",
    "\n",
    "  open_file_object.writerows(zip(indices, uniqueLabels[np.argmax(output, 1)]))\n",
    "  predictions_file.close()\n",
    "#session.close()\n",
    "\n",
    "#  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPH produced with TensorBoard:\n",
    "\n",
    "<img src=\"files/Graph_FirstLayerConvolution.png\">\n",
    "\n",
    "## Results:\n",
    "# Loss \n",
    "<img src=\"files/loss_FirstLayerConvolution.png\">\n",
    "The loss as function of step number (x axis) during training. It drops oh.. so smoothly! \n",
    "\n",
    "# Weights of first layer (convolution2D)\n",
    "\n",
    "<img src=\"files/FirstLayer_weight_convolution.png\">\n",
    "This plot shows the weight values in the vertical axis as they change for every iteration of the training shown in the x axis. In this case and for all weight distribution I get in this project, the distributions are gaussian centered around 0 and get wider as the training progresses.\n",
    "\n",
    "# Biases at first layer\n",
    "\n",
    "<img src=\"files/FirstLayer_bias_convolution.png\">\n",
    "\n",
    "Not much to say here besides the obvious fact that biases values are changing. Worrysome sharp discontinuities.\n",
    "\n",
    "<img src=\"files/Layer2_weights_convolution.png\">\n",
    "\n",
    "<img src=\"files/Layer3_weights_convolution.png\">\n",
    "\n",
    "<img src=\"files/FullyConnected_weights_convolution.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
